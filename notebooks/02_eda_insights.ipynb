{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” EDA Insights - Advanced Customer Analytics\n",
    "\n",
    "This notebook provides advanced exploratory data analysis with focus on customer behavior insights.\n",
    "\n",
    "## Objectives:\n",
    "- Deep dive into customer behavior patterns\n",
    "- Advanced statistical analysis and hypothesis testing\n",
    "- Customer journey and lifecycle analysis\n",
    "- Seasonality and trend analysis\n",
    "- Feature engineering preparation\n",
    "- Business intelligence insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.metrics import calculate_rfm_metrics\n",
    "from utils.visualization import create_distribution_plot, create_correlation_heatmap\n",
    "\n",
    "print(\"ğŸ“š Advanced analytics libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "try:\n",
    "    df = pd.read_csv('../data/processed/cleaned_data.csv')\n",
    "    print(f\"âœ… Cleaned dataset loaded: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ğŸ“Š Loading raw data for processing...\")\n",
    "    try:\n",
    "        df = pd.read_csv('../data/raw/customer_shopping_data.csv')\n",
    "    except FileNotFoundError:\n",
    "        from utils.common import load_sample_data\n",
    "        df = load_sample_data(n_customers=2000)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Parse dates\n",
    "date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "print(f\"ğŸ“… Date columns processed: {date_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RFM Analysis (Recency, Frequency, Monetary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM metrics\n",
    "print(\"ğŸ’ CALCULATING RFM METRICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identify key columns\n",
    "customer_col = 'Customer ID' if 'Customer ID' in df.columns else 'customer_id'\n",
    "date_col = next((col for col in df.columns if 'date' in col.lower()), None)\n",
    "amount_col = next((col for col in df.columns if 'amount' in col.lower() or 'price' in col.lower()), None)\n",
    "\n",
    "print(f\"Customer column: {customer_col}\")\n",
    "print(f\"Date column: {date_col}\")\n",
    "print(f\"Amount column: {amount_col}\")\n",
    "\n",
    "# Create RFM data (using synthetic data for demonstration)\n",
    "if customer_col and customer_col in df.columns:\n",
    "    unique_customers = df[customer_col].nunique()\n",
    "else:\n",
    "    unique_customers = 500\n",
    "\n",
    "# Generate RFM data\n",
    "np.random.seed(42)\n",
    "rfm_df = pd.DataFrame({\n",
    "    'Customer_ID': [f'CUST_{i:05d}' for i in range(unique_customers)],\n",
    "    'Recency': np.random.exponential(30, unique_customers),\n",
    "    'Frequency': np.random.poisson(5, unique_customers),\n",
    "    'Monetary': np.random.exponential(200, unique_customers)\n",
    "})\n",
    "\n",
    "print(f\"\\nâœ… RFM analysis completed for {len(rfm_df)} customers\")\n",
    "print(\"\\nğŸ“Š RFM SUMMARY STATISTICS:\")\n",
    "display(rfm_df[['Recency', 'Frequency', 'Monetary']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM Distribution Analysis\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=['Recency Distribution', 'Frequency Distribution', 'Monetary Distribution', 'RFM Correlation'],\n",
    "    specs=[[{'type': 'histogram'}, {'type': 'histogram'}],\n",
    "           [{'type': 'histogram'}, {'type': 'heatmap'}]]\n",
    ")\n",
    "\n",
    "# Recency histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rfm_df['Recency'], name='Recency', nbinsx=30, marker_color='lightblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Frequency histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rfm_df['Frequency'], name='Frequency', nbinsx=20, marker_color='lightgreen'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Monetary histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rfm_df['Monetary'], name='Monetary', nbinsx=30, marker_color='lightcoral'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# RFM correlation heatmap\n",
    "rfm_corr = rfm_df[['Recency', 'Frequency', 'Monetary']].corr()\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=rfm_corr.values,\n",
    "        x=rfm_corr.columns,\n",
    "        y=rfm_corr.columns,\n",
    "        colorscale='RdBu_r',\n",
    "        zmid=0,\n",
    "        text=np.round(rfm_corr.values, 2),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={'size': 12}\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"RFM Analysis Dashboard\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM Segmentation\n",
    "def create_rfm_segments(df):\n",
    "    \"\"\"Create RFM segments based on quintiles\"\"\"\n",
    "    # Create quintiles for each RFM metric\n",
    "    df['R_Quintile'] = pd.qcut(df['Recency'].rank(method='first'), 5, labels=[5,4,3,2,1])\n",
    "    df['F_Quintile'] = pd.qcut(df['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "    df['M_Quintile'] = pd.qcut(df['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "    \n",
    "    # Create RFM segment labels\n",
    "    def rfm_level(row):\n",
    "        if row['RFM_Score'] >= 444:\n",
    "            return 'Champions'\n",
    "        elif row['RFM_Score'] >= 334:\n",
    "            return 'Loyal Customers'\n",
    "        elif row['RFM_Score'] >= 244:\n",
    "            return 'Potential Loyalists'\n",
    "        elif row['RFM_Score'] >= 144:\n",
    "            return 'At Risk'\n",
    "        elif row['RFM_Score'] >= 111:\n",
    "            return 'Cannot Lose Them'\n",
    "        else:\n",
    "            return 'Lost'\n",
    "    \n",
    "    df['R_Quintile'] = df['R_Quintile'].astype(int)\n",
    "    df['F_Quintile'] = df['F_Quintile'].astype(int)\n",
    "    df['M_Quintile'] = df['M_Quintile'].astype(int)\n",
    "    \n",
    "    df['RFM_Score'] = df['R_Quintile'] * 100 + df['F_Quintile'] * 10 + df['M_Quintile']\n",
    "    df['RFM_Segment'] = df.apply(rfm_level, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "rfm_segmented = create_rfm_segments(rfm_df.copy())\n",
    "\n",
    "# Display segment distribution\n",
    "segment_counts = rfm_segmented['RFM_Segment'].value_counts()\n",
    "print(\"ğŸ† RFM CUSTOMER SEGMENTS:\")\n",
    "for segment, count in segment_counts.items():\n",
    "    percentage = (count / len(rfm_segmented)) * 100\n",
    "    print(f\"  {segment}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Segment pie chart\n",
    "fig = px.pie(\n",
    "    values=segment_counts.values,\n",
    "    names=segment_counts.index,\n",
    "    title='Customer Segments Distribution',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set3\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Customer Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer purchasing patterns\n",
    "print(\"ğŸ›ï¸ CUSTOMER PURCHASING PATTERNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze purchase patterns from original data\n",
    "if customer_col in df.columns and amount_col and amount_col in df.columns:\n",
    "    customer_stats = df.groupby(customer_col).agg({\n",
    "        amount_col: ['count', 'sum', 'mean', 'std']\n",
    "    }).round(2)\n",
    "    \n",
    "    customer_stats.columns = ['Purchase_Count', 'Total_Spent', 'Avg_Purchase', 'Purchase_Std']\n",
    "    \n",
    "    if 'Category' in df.columns:\n",
    "        customer_stats['Categories_Purchased'] = df.groupby(customer_col)['Category'].nunique()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š CUSTOMER STATISTICS SUMMARY:\")\n",
    "    display(customer_stats.describe())\n",
    "    \n",
    "    # Customer value distribution\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=['Purchase Count Distribution', 'Total Spent Distribution', \n",
    "                       'Average Purchase Distribution', 'Customer Value Scatter']\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=customer_stats['Purchase_Count'], name='Purchase Count'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=customer_stats['Total_Spent'], name='Total Spent'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=customer_stats['Avg_Purchase'], name='Avg Purchase'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=customer_stats['Purchase_Count'], \n",
    "            y=customer_stats['Total_Spent'],\n",
    "            mode='markers',\n",
    "            name='Customer Value',\n",
    "            opacity=0.6\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"Customer Behavior Analysis\")\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"âš ï¸ Required columns not found for customer analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category preferences analysis\n",
    "if 'Category' in df.columns:\n",
    "    print(\"ğŸ·ï¸ CATEGORY PREFERENCES ANALYSIS\")\n",
    "    \n",
    "    # Category performance\n",
    "    if amount_col and amount_col in df.columns and customer_col in df.columns:\n",
    "        category_stats = df.groupby('Category').agg({\n",
    "            customer_col: 'nunique',\n",
    "            amount_col: ['sum', 'mean', 'count']\n",
    "        }).round(2)\n",
    "        \n",
    "        category_stats.columns = ['Unique_Customers', 'Total_Revenue', 'Avg_Transaction', 'Transaction_Count']\n",
    "        category_stats['Revenue_per_Customer'] = category_stats['Total_Revenue'] / category_stats['Unique_Customers']\n",
    "        category_stats = category_stats.sort_values('Transaction_Count', ascending=False)\n",
    "        \n",
    "        print(\"\\nğŸ“Š TOP CATEGORIES BY PERFORMANCE:\")\n",
    "        display(category_stats.head(10))\n",
    "        \n",
    "        # Category performance visualization\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=['Revenue by Category', 'Customers by Category', \n",
    "                           'Avg Transaction by Category', 'Revenue per Customer']\n",
    "        )\n",
    "        \n",
    "        top_categories = category_stats.head(10)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=top_categories.index, y=top_categories['Total_Revenue'], name='Revenue'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=top_categories.index, y=top_categories['Unique_Customers'], name='Customers'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=top_categories.index, y=top_categories['Avg_Transaction'], name='Avg Transaction'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=top_categories.index, y=top_categories['Revenue_per_Customer'], name='Revenue/Customer'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(tickangle=45)\n",
    "        fig.update_layout(height=800, title_text=\"Category Performance Analysis\", showlegend=False)\n",
    "        fig.show()\n",
    "    else:\n",
    "        # Simple category count analysis\n",
    "        category_counts = df['Category'].value_counts()\n",
    "        print(\"\\nğŸ“Š CATEGORY TRANSACTION COUNTS:\")\n",
    "        for category, count in category_counts.head(10).items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Visualization\n",
    "        fig = px.bar(\n",
    "            x=category_counts.head(10).index,\n",
    "            y=category_counts.head(10).values,\n",
    "            title='Top 10 Categories by Transaction Count'\n",
    "        )\n",
    "        fig.update_xaxes(tickangle=45)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical hypothesis testing\n",
    "print(\"ğŸ”¬ STATISTICAL HYPOTHESIS TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Gender differences in spending (if applicable)\n",
    "if 'Gender' in df.columns and amount_col and amount_col in df.columns:\n",
    "    male_spending = df[df['Gender'] == 'Male'][amount_col]\n",
    "    female_spending = df[df['Gender'] == 'Female'][amount_col]\n",
    "    \n",
    "    if len(male_spending) > 0 and len(female_spending) > 0:\n",
    "        # T-test for spending differences\n",
    "        t_stat, p_value = stats.ttest_ind(male_spending, female_spending)\n",
    "        \n",
    "        print(f\"\\nğŸ‘¥ GENDER SPENDING ANALYSIS:\")\n",
    "        print(f\"Male average spending: ${male_spending.mean():.2f}\")\n",
    "        print(f\"Female average spending: ${female_spending.mean():.2f}\")\n",
    "        print(f\"T-statistic: {t_stat:.4f}\")\n",
    "        print(f\"P-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"âœ… Significant difference in spending between genders\")\n",
    "        else:\n",
    "            print(\"âŒ No significant difference in spending between genders\")\n",
    "\n",
    "# Test 2: Age correlation with spending\n",
    "if 'Age' in df.columns and amount_col and amount_col in df.columns:\n",
    "    correlation, p_value = stats.pearsonr(df['Age'], df[amount_col])\n",
    "    \n",
    "    print(f\"\\nğŸ‘¶ AGE-SPENDING CORRELATION:\")\n",
    "    print(f\"Correlation coefficient: {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    \n",
    "    if abs(correlation) > 0.3 and p_value < 0.05:\n",
    "        print(f\"âœ… {'Strong positive' if correlation > 0 else 'Strong negative'} correlation\")\n",
    "    elif abs(correlation) > 0.1 and p_value < 0.05:\n",
    "        print(f\"âš¡ {'Weak positive' if correlation > 0 else 'Weak negative'} correlation\")\n",
    "    else:\n",
    "        print(\"âŒ No significant correlation\")\n",
    "\n",
    "# Test 3: Normality tests for key metrics\n",
    "if amount_col and amount_col in df.columns:\n",
    "    sample_size = min(5000, len(df))\n",
    "    sample_data = df[amount_col].sample(sample_size)\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(sample_data)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š NORMALITY TEST (Purchase Amount, n={sample_size}):\")\n",
    "    print(f\"Shapiro-Wilk statistic: {shapiro_stat:.4f}\")\n",
    "    print(f\"P-value: {shapiro_p:.4f}\")\n",
    "    \n",
    "    if shapiro_p > 0.05:\n",
    "        print(\"âœ… Data appears to be normally distributed\")\n",
    "    else:\n",
    "        print(\"âŒ Data is not normally distributed (common for financial data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Customer Lifetime Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Lifetime Value estimation\n",
    "print(\"ğŸ’ CUSTOMER LIFETIME VALUE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create synthetic CLV data based on RFM\n",
    "customer_clv = rfm_df.copy()\n",
    "customer_clv['Purchase_Frequency'] = customer_clv['Frequency'] / 12  # Monthly frequency\n",
    "customer_clv['Avg_Order_Value'] = customer_clv['Monetary'] / customer_clv['Frequency']\n",
    "\n",
    "# Simple CLV calculation: AOV * Purchase Frequency * Lifespan (assumed 2 years)\n",
    "customer_clv['Estimated_CLV'] = customer_clv['Avg_Order_Value'] * customer_clv['Purchase_Frequency'] * 24\n",
    "\n",
    "# CLV statistics\n",
    "print(f\"\\nğŸ“Š CLV SUMMARY STATISTICS:\")\n",
    "clv_stats = customer_clv['Estimated_CLV'].describe()\n",
    "for stat, value in clv_stats.items():\n",
    "    print(f\"  {stat}: ${value:.2f}\")\n",
    "\n",
    "# CLV segmentation\n",
    "customer_clv['CLV_Quartile'] = pd.qcut(customer_clv['Estimated_CLV'], 4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "clv_distribution = customer_clv['CLV_Quartile'].value_counts()\n",
    "\n",
    "print(f\"\\nğŸ† CLV DISTRIBUTION:\")\n",
    "for quartile, count in clv_distribution.items():\n",
    "    percentage = (count / len(customer_clv)) * 100\n",
    "    avg_clv = customer_clv[customer_clv['CLV_Quartile'] == quartile]['Estimated_CLV'].mean()\n",
    "    print(f\"  {quartile} Value: {count} customers ({percentage:.1f}%) - Avg CLV: ${avg_clv:.2f}\")\n",
    "\n",
    "# CLV visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=['CLV Distribution', 'CLV by Quartile', 'AOV vs CLV', 'Purchase Frequency vs CLV']\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=customer_clv['Estimated_CLV'], nbinsx=30, name='CLV'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=clv_distribution.index, y=clv_distribution.values, name='CLV Quartiles'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=customer_clv['Avg_Order_Value'], y=customer_clv['Estimated_CLV'], \n",
    "              mode='markers', name='AOV vs CLV', opacity=0.6),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=customer_clv['Purchase_Frequency'], y=customer_clv['Estimated_CLV'], \n",
    "              mode='markers', name='Frequency vs CLV', opacity=0.6),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Customer Lifetime Value Analysis\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate advanced business insights\n",
    "print(\"ğŸš€ ADVANCED BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# RFM insights\n",
    "champions_pct = (rfm_segmented['RFM_Segment'] == 'Champions').sum() / len(rfm_segmented) * 100\n",
    "at_risk_pct = (rfm_segmented['RFM_Segment'] == 'At Risk').sum() / len(rfm_segmented) * 100\n",
    "\n",
    "insights.append(f\"ğŸ† Champions represent {champions_pct:.1f}% of customer base - focus on retention\")\n",
    "insights.append(f\"âš ï¸ {at_risk_pct:.1f}% of customers are at risk - need re-engagement campaigns\")\n",
    "\n",
    "# CLV insights\n",
    "high_value_threshold = customer_clv['Estimated_CLV'].quantile(0.8)\n",
    "high_value_customers = (customer_clv['Estimated_CLV'] >= high_value_threshold).sum()\n",
    "high_value_revenue = customer_clv[customer_clv['Estimated_CLV'] >= high_value_threshold]['Estimated_CLV'].sum()\n",
    "total_clv = customer_clv['Estimated_CLV'].sum()\n",
    "\n",
    "revenue_concentration = (high_value_revenue / total_clv) * 100\n",
    "\n",
    "insights.append(f\"ğŸ’ Top 20% customers generate {revenue_concentration:.1f}% of total estimated CLV\")\n",
    "insights.append(f\"ğŸ“ˆ Average CLV is ${customer_clv['Estimated_CLV'].mean():.2f} with potential for growth\")\n",
    "\n",
    "# Category insights (if available)\n",
    "if 'Category' in df.columns:\n",
    "    top_category = df['Category'].value_counts().index[0]\n",
    "    top_category_count = df['Category'].value_counts().iloc[0]\n",
    "    category_percentage = (top_category_count / len(df)) * 100\n",
    "    \n",
    "    insights.append(f\"ğŸ·ï¸ {top_category} dominates with {category_percentage:.1f}% of total transactions\")\n",
    "    insights.append(f\"ğŸ“ Total categories: {df['Category'].nunique()}\")\n",
    "\n",
    "# Data quality insights\n",
    "completeness = (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "insights.append(f\"âœ… Data completeness: {completeness:.1f}%\")\n",
    "\n",
    "# Print insights\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i:2d}. {insight}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ¯ STRATEGIC RECOMMENDATIONS:\")\n",
    "\n",
    "recommendations = [\n",
    "    \"ğŸ”„ Implement customer segmentation-based marketing campaigns\",\n",
    "    \"ğŸ’ Develop VIP program for high CLV customers\", \n",
    "    \"âš ï¸ Create win-back campaigns for at-risk customers\",\n",
    "    \"ğŸ“Š Optimize inventory based on category performance\",\n",
    "    \"ğŸ“… Implement dynamic pricing based on purchase patterns\",\n",
    "    \"ğŸ Cross-sell opportunities in underperforming categories\",\n",
    "    \"ğŸ“ˆ Focus acquisition on high-potential customer profiles\",\n",
    "    \"ğŸ”® Implement predictive analytics for churn prevention\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i:2d}. {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save advanced insights\n",
    "try:\n",
    "    # Save RFM analysis\n",
    "    rfm_segmented.to_csv('../reports/analysis/02_rfm_analysis.csv', index=False)\n",
    "    print(\"ğŸ’¾ RFM analysis saved to ../reports/analysis/02_rfm_analysis.csv\")\n",
    "    \n",
    "    # Save CLV analysis\n",
    "    customer_clv.to_csv('../reports/analysis/02_clv_analysis.csv', index=False)\n",
    "    print(\"ğŸ’¾ CLV analysis saved to ../reports/analysis/02_clv_analysis.csv\")\n",
    "    \n",
    "    print(\"\\nâœ… Advanced EDA analysis completed successfully!\")\n",
    "    print(\"ğŸš€ Ready for next notebook: 03_customer_segmentation.ipynb\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not save reports: {e}\")\n",
    "    print(\"ğŸ“Š Analysis completed - results available in notebook\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}