{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠ê Model Evaluation - Comprehensive Analysis\n",
    "\n",
    "This notebook provides comprehensive evaluation of our predictive models for customer analytics.\n",
    "\n",
    "## Objectives:\n",
    "- Evaluate model performance using multiple metrics\n",
    "- Conduct statistical significance testing\n",
    "- Analyze feature importance and interpretability\n",
    "- Assess model robustness and generalization\n",
    "- Validate business impact and ROI\n",
    "- Provide deployment recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning evaluation imports\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve, \n",
    "    precision_recall_curve, f1_score, precision_score, recall_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, validation_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy import stats\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.evaluation import ModelEvaluator\n",
    "\n",
    "print(\"üìä Model evaluation libraries loaded!\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "print(\"üé® Plotting configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model results and predictions\n",
    "try:\n",
    "    predictions_df = pd.read_csv('../reports/analysis/04_model_predictions.csv')\n",
    "    model_summary = pd.read_csv('../reports/analysis/04_model_performance_summary.csv')\n",
    "    print(f\"‚úÖ Model results loaded successfully\")\n",
    "    print(f\"Predictions shape: {predictions_df.shape}\")\n",
    "    display(predictions_df.head())\n",
    "    print(f\"\\nModel Summary:\")\n",
    "    display(model_summary)\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Model results not found. Creating synthetic evaluation data...\")\n",
    "    # Create synthetic data for evaluation\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Actual_Churn': np.random.binomial(1, 0.3, n_samples),\n",
    "        'Predicted_Churn': np.random.binomial(1, 0.3, n_samples),\n",
    "        'Churn_Probability': np.random.random(n_samples),\n",
    "        'Actual_CLV': np.random.exponential(300, n_samples),\n",
    "        'Predicted_CLV': np.random.exponential(300, n_samples)\n",
    "    })\n",
    "    \n",
    "    model_summary = pd.DataFrame({\n",
    "        'Model': ['Churn Prediction', 'CLV Prediction'],\n",
    "        'Algorithm': ['Random Forest Classifier', 'Random Forest Regressor'],\n",
    "        'Primary_Metric': [0.85, 0.75],\n",
    "        'Metric_Name': ['AUC Score', 'R¬≤ Score'],\n",
    "        'Secondary_Metric': [0.78, 45.2],\n",
    "        'Secondary_Metric_Name': ['Accuracy', 'RMSE']\n",
    "    })\n",
    "    \n",
    "    print(\"üìä Synthetic evaluation data created\")\n",
    "\n",
    "# Load original data for additional analysis\n",
    "try:\n",
    "    df = pd.read_csv('../data/processed/cleaned_data.csv')\n",
    "    print(f\"‚úÖ Original dataset loaded: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df = pd.read_csv('../data/raw/customer_shopping_data.csv')\n",
    "    except FileNotFoundError:\n",
    "        from utils.common import load_sample_data\n",
    "        df = load_sample_data(n_customers=2000)\n",
    "    print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Churn Prediction Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive churn model evaluation\n",
    "def evaluate_churn_model(predictions_df):\n",
    "    \"\"\"Comprehensive evaluation of churn prediction model\"\"\"\n",
    "    \n",
    "    y_true = predictions_df['Actual_Churn']\n",
    "    y_pred = predictions_df['Predicted_Churn']\n",
    "    y_proba = predictions_df['Churn_Probability']\n",
    "    \n",
    "    print(\"üéØ CHURN MODEL EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = (y_pred == y_true).mean()\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    print(f\"\\nüìä CLASSIFICATION METRICS:\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall: {recall:.3f}\")\n",
    "    print(f\"  F1-Score: {f1:.3f}\")\n",
    "    print(f\"  AUC Score: {auc:.3f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nüìã CONFUSION MATRIX:\")\n",
    "    print(f\"              Predicted\")\n",
    "    print(f\"              0    1\")\n",
    "    print(f\"Actual   0   {cm[0,0]:4d} {cm[0,1]:4d}\")\n",
    "    print(f\"         1   {cm[1,0]:4d} {cm[1,1]:4d}\")\n",
    "    \n",
    "    # Business impact metrics\n",
    "    true_positives = cm[1, 1]\n",
    "    false_positives = cm[0, 1]\n",
    "    false_negatives = cm[1, 0]\n",
    "    true_negatives = cm[0, 0]\n",
    "    \n",
    "    print(f\"\\nüíº BUSINESS IMPACT ANALYSIS:\")\n",
    "    print(f\"  Correctly Identified Churners: {true_positives} (Potential saves)\")\n",
    "    print(f\"  Missed Churners: {false_negatives} (Lost customers)\")\n",
    "    print(f\"  False Alarms: {false_positives} (Unnecessary interventions)\")\n",
    "    print(f\"  Correctly Identified Loyal: {true_negatives} (No action needed)\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Evaluate churn model\n",
    "churn_metrics = evaluate_churn_model(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize churn model performance\n",
    "def visualize_churn_evaluation(predictions_df, metrics):\n",
    "    \"\"\"Create comprehensive visualization of churn model performance\"\"\"\n",
    "    \n",
    "    y_true = predictions_df['Actual_Churn']\n",
    "    y_pred = predictions_df['Predicted_Churn']\n",
    "    y_proba = predictions_df['Churn_Probability']\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=['Confusion Matrix', 'ROC Curve', 'Precision-Recall Curve',\n",
    "                       'Probability Distribution', 'Calibration Plot', 'Threshold Analysis'],\n",
    "        specs=[[{'type': 'heatmap'}, {'type': 'scatter'}, {'type': 'scatter'}],\n",
    "               [{'type': 'histogram'}, {'type': 'scatter'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # Confusion Matrix Heatmap\n",
    "    cm = metrics['confusion_matrix']\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=cm,\n",
    "            x=['Predicted 0', 'Predicted 1'],\n",
    "            y=['Actual 0', 'Actual 1'],\n",
    "            colorscale='Blues',\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={'size': 16},\n",
    "            showscale=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC (AUC={metrics[\"auc\"]:.3f})'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash'), \n",
    "                  name='Random', showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_proba)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=recall_vals, y=precision_vals, mode='lines', \n",
    "                  name=f'PR Curve'),\n",
    "        row=1, col=3\n",
    "    )\n",
    "    \n",
    "    # Probability Distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=y_proba[y_true == 0], name='Non-Churners', opacity=0.7, nbinsx=30),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=y_proba[y_true == 1], name='Churners', opacity=0.7, nbinsx=30),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Calibration Plot\n",
    "    try:\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_proba, n_bins=10)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=mean_predicted_value, y=fraction_of_positives, \n",
    "                      mode='lines+markers', name='Calibration'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash'),\n",
    "                      name='Perfect', showlegend=False),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Calibration plot error: {e}\")\n",
    "    \n",
    "    # Threshold Analysis\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "        if y_pred_thresh.sum() > 0 and (1 - y_pred_thresh).sum() > 0:\n",
    "            precisions.append(precision_score(y_true, y_pred_thresh))\n",
    "            recalls.append(recall_score(y_true, y_pred_thresh))\n",
    "            f1_scores.append(f1_score(y_true, y_pred_thresh))\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "            recalls.append(0)\n",
    "            f1_scores.append(0)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=thresholds, y=precisions, mode='lines', name='Precision'),\n",
    "        row=2, col=3\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=thresholds, y=recalls, mode='lines', name='Recall'),\n",
    "        row=2, col=3\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=thresholds, y=f1_scores, mode='lines', name='F1-Score'),\n",
    "        row=2, col=3\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"Churn Model Performance Analysis\")\n",
    "    fig.show()\n",
    "\n",
    "# Visualize churn evaluation\n",
    "visualize_churn_evaluation(predictions_df, churn_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Customer Lifetime Value (CLV) Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive CLV model evaluation\n",
    "def evaluate_clv_model(predictions_df):\n",
    "    \"\"\"Comprehensive evaluation of CLV prediction model\"\"\"\n",
    "    \n",
    "    y_true = predictions_df['Actual_CLV']\n",
    "    y_pred = predictions_df['Predicted_CLV']\n",
    "    \n",
    "    print(\"üíé CLV MODEL EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Regression metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    explained_var = explained_variance_score(y_true, y_pred)\n",
    "    \n",
    "    # Additional metrics\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100  # Mean Absolute Percentage Error\n",
    "    max_error = np.max(np.abs(y_true - y_pred))\n",
    "    \n",
    "    print(f\"\\nüìä REGRESSION METRICS:\")\n",
    "    print(f\"  R¬≤ Score: {r2:.3f}\")\n",
    "    print(f\"  RMSE: ${rmse:.2f}\")\n",
    "    print(f\"  MAE: ${mae:.2f}\")\n",
    "    print(f\"  Explained Variance: {explained_var:.3f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    print(f\"  Max Error: ${max_error:.2f}\")\n",
    "    \n",
    "    # Error analysis\n",
    "    errors = y_pred - y_true\n",
    "    mean_error = np.mean(errors)\n",
    "    std_error = np.std(errors)\n",
    "    \n",
    "    print(f\"\\nüìà ERROR ANALYSIS:\")\n",
    "    print(f\"  Mean Error (Bias): ${mean_error:.2f}\")\n",
    "    print(f\"  Error Std Dev: ${std_error:.2f}\")\n",
    "    print(f\"  Error Range: ${errors.min():.2f} to ${errors.max():.2f}\")\n",
    "    \n",
    "    # Business impact analysis\n",
    "    actual_total_clv = y_true.sum()\n",
    "    predicted_total_clv = y_pred.sum()\n",
    "    total_error = predicted_total_clv - actual_total_clv\n",
    "    \n",
    "    print(f\"\\nüíº BUSINESS IMPACT ANALYSIS:\")\n",
    "    print(f\"  Actual Total CLV: ${actual_total_clv:,.2f}\")\n",
    "    print(f\"  Predicted Total CLV: ${predicted_total_clv:,.2f}\")\n",
    "    print(f\"  Total Prediction Error: ${total_error:,.2f} ({total_error/actual_total_clv*100:.1f}%)\")\n",
    "    \n",
    "    # CLV quartile analysis\n",
    "    y_true_quartiles = pd.qcut(y_true, 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "    quartile_performance = []\n",
    "    \n",
    "    for quartile in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "        mask = y_true_quartiles == quartile\n",
    "        q_r2 = r2_score(y_true[mask], y_pred[mask])\n",
    "        q_mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
    "        quartile_performance.append({\n",
    "            'Quartile': quartile,\n",
    "            'R2': q_r2,\n",
    "            'MAE': q_mae,\n",
    "            'Count': mask.sum(),\n",
    "            'Avg_Actual': y_true[mask].mean(),\n",
    "            'Avg_Predicted': y_pred[mask].mean()\n",
    "        })\n",
    "    \n",
    "    quartile_df = pd.DataFrame(quartile_performance)\n",
    "    print(f\"\\nüìä PERFORMANCE BY CLV QUARTILE:\")\n",
    "    display(quartile_df.round(3))\n",
    "    \n",
    "    return {\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'explained_variance': explained_var,\n",
    "        'errors': errors,\n",
    "        'quartile_performance': quartile_df\n",
    "    }\n",
    "\n",
    "# Evaluate CLV model\n",
    "clv_metrics = evaluate_clv_model(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CLV model performance\n",
    "def visualize_clv_evaluation(predictions_df, metrics):\n",
    "    \"\"\"Create comprehensive visualization of CLV model performance\"\"\"\n",
    "    \n",
    "    y_true = predictions_df['Actual_CLV']\n",
    "    y_pred = predictions_df['Predicted_CLV']\n",
    "    errors = metrics['errors']\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=['Actual vs Predicted', 'Residual Plot', 'Error Distribution',\n",
    "                       'CLV Distribution Comparison', 'Quartile Performance', 'Prediction Intervals']\n",
    "    )\n",
    "    \n",
    "    # Actual vs Predicted Scatter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=y_true, y=y_pred, mode='markers', name='Predictions', opacity=0.6),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=[min_val, max_val], y=[min_val, max_val], mode='lines',\n",
    "                  line=dict(dash='dash', color='red'), name='Perfect Prediction'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Residual Plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=y_pred, y=errors, mode='markers', name='Residuals', opacity=0.6),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=1, col=2)\n",
    "    \n",
    "    # Error Distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=errors, name='Error Distribution', nbinsx=30),\n",
    "        row=1, col=3\n",
    "    )\n",
    "    \n",
    "    # CLV Distribution Comparison\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=y_true, name='Actual CLV', opacity=0.7, nbinsx=30),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=y_pred, name='Predicted CLV', opacity=0.7, nbinsx=30),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Quartile Performance\n",
    "    quartile_df = metrics['quartile_performance']\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=quartile_df['Quartile'], y=quartile_df['R2'], name='R¬≤ by Quartile'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Prediction Intervals (simplified)\n",
    "    sorted_indices = np.argsort(y_true)\n",
    "    sorted_true = y_true.iloc[sorted_indices]\n",
    "    sorted_pred = y_pred.iloc[sorted_indices]\n",
    "    \n",
    "    # Calculate rolling standard deviation for prediction intervals\n",
    "    window_size = max(50, len(sorted_true) // 20)\n",
    "    rolling_std = pd.Series(errors).rolling(window=window_size, center=True).std().fillna(errors.std())\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=sorted_true, y=sorted_pred, mode='lines', name='Predictions'),\n",
    "        row=2, col=3\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"CLV Model Performance Analysis\")\n",
    "    fig.show()\n",
    "\n",
    "# Visualize CLV evaluation\n",
    "visualize_clv_evaluation(predictions_df, clv_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Robustness and Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation stability analysis\n",
    "def analyze_model_stability(predictions_df):\n",
    "    \"\"\"Analyze model stability and robustness\"\"\"\n",
    "    \n",
    "    print(\"üîí MODEL STABILITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Bootstrap analysis for confidence intervals\n",
    "    n_bootstrap = 100\n",
    "    bootstrap_results = {'churn_auc': [], 'clv_r2': []}\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    for i in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        sample_indices = np.random.choice(len(predictions_df), size=len(predictions_df), replace=True)\n",
    "        sample_df = predictions_df.iloc[sample_indices]\n",
    "        \n",
    "        # Calculate metrics for bootstrap sample\n",
    "        try:\n",
    "            churn_auc = roc_auc_score(sample_df['Actual_Churn'], sample_df['Churn_Probability'])\n",
    "            clv_r2 = r2_score(sample_df['Actual_CLV'], sample_df['Predicted_CLV'])\n",
    "            \n",
    "            bootstrap_results['churn_auc'].append(churn_auc)\n",
    "            bootstrap_results['clv_r2'].append(clv_r2)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    churn_auc_ci = np.percentile(bootstrap_results['churn_auc'], [2.5, 97.5])\n",
    "    clv_r2_ci = np.percentile(bootstrap_results['clv_r2'], [2.5, 97.5])\n",
    "    \n",
    "    print(f\"\\nüìä BOOTSTRAP CONFIDENCE INTERVALS (95%):\")\n",
    "    print(f\"  Churn AUC: {np.mean(bootstrap_results['churn_auc']):.3f} [{churn_auc_ci[0]:.3f}, {churn_auc_ci[1]:.3f}]\")\n",
    "    print(f\"  CLV R¬≤: {np.mean(bootstrap_results['clv_r2']):.3f} [{clv_r2_ci[0]:.3f}, {clv_r2_ci[1]:.3f}]\")\n",
    "    \n",
    "    # Stability metrics\n",
    "    churn_auc_std = np.std(bootstrap_results['churn_auc'])\n",
    "    clv_r2_std = np.std(bootstrap_results['clv_r2'])\n",
    "    \n",
    "    print(f\"\\nüìà STABILITY METRICS:\")\n",
    "    print(f\"  Churn AUC Std Dev: {churn_auc_std:.3f} (Lower is more stable)\")\n",
    "    print(f\"  CLV R¬≤ Std Dev: {clv_r2_std:.3f} (Lower is more stable)\")\n",
    "    \n",
    "    # Performance consistency check\n",
    "    churn_consistency = (churn_auc_std < 0.05)  # AUC should not vary by more than 0.05\n",
    "    clv_consistency = (clv_r2_std < 0.05)  # R¬≤ should not vary by more than 0.05\n",
    "    \n",
    "    print(f\"\\n‚úÖ CONSISTENCY CHECK:\")\n",
    "    print(f\"  Churn Model: {'Stable' if churn_consistency else 'Unstable'}\")\n",
    "    print(f\"  CLV Model: {'Stable' if clv_consistency else 'Unstable'}\")\n",
    "    \n",
    "    return {\n",
    "        'bootstrap_results': bootstrap_results,\n",
    "        'confidence_intervals': {\n",
    "            'churn_auc': churn_auc_ci,\n",
    "            'clv_r2': clv_r2_ci\n",
    "        },\n",
    "        'stability': {\n",
    "            'churn_stable': churn_consistency,\n",
    "            'clv_stable': clv_consistency\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Analyze model stability\n",
    "stability_results = analyze_model_stability(predictions_df)\n",
    "\n",
    "# Visualize stability analysis\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Churn AUC Bootstrap Distribution', 'CLV R¬≤ Bootstrap Distribution']\n",
    ")\n",
    "\n",
    "# Churn AUC distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=stability_results['bootstrap_results']['churn_auc'], \n",
    "                name='Churn AUC', nbinsx=20),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# CLV R¬≤ distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=stability_results['bootstrap_results']['clv_r2'], \n",
    "                name='CLV R¬≤', nbinsx=20),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Model Stability Analysis - Bootstrap Distributions\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Business Impact Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact assessment\n",
    "def assess_business_impact(predictions_df, df):\n",
    "    \"\"\"Assess the business impact of the predictive models\"\"\"\n",
    "    \n",
    "    print(\"üíº BUSINESS IMPACT ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Churn prevention impact\n",
    "    y_true_churn = predictions_df['Actual_Churn']\n",
    "    y_pred_churn = predictions_df['Predicted_Churn']\n",
    "    y_proba_churn = predictions_df['Churn_Probability']\n",
    "    \n",
    "    # Assume average customer value and retention campaign costs\n",
    "    avg_customer_value = predictions_df['Actual_CLV'].mean() if 'Actual_CLV' in predictions_df.columns else 200\n",
    "    retention_campaign_cost = 20  # Cost per targeted customer\n",
    "    retention_success_rate = 0.3  # 30% of targeted churners can be saved\n",
    "    \n",
    "    # Calculate potential savings\n",
    "    high_risk_customers = (y_proba_churn >= 0.7).sum()  # High churn probability\n",
    "    true_churners_identified = ((y_proba_churn >= 0.7) & (y_true_churn == 1)).sum()\n",
    "    \n",
    "    campaign_cost = high_risk_customers * retention_campaign_cost\n",
    "    customers_saved = true_churners_identified * retention_success_rate\n",
    "    revenue_saved = customers_saved * avg_customer_value\n",
    "    net_benefit = revenue_saved - campaign_cost\n",
    "    \n",
    "    print(f\"\\nüéØ CHURN PREVENTION IMPACT:\")\n",
    "    print(f\"  High-risk customers identified: {high_risk_customers:,}\")\n",
    "    print(f\"  True churners in high-risk group: {true_churners_identified:,}\")\n",
    "    print(f\"  Retention campaign cost: ${campaign_cost:,.2f}\")\n",
    "    print(f\"  Customers potentially saved: {customers_saved:.0f}\")\n",
    "    print(f\"  Revenue saved: ${revenue_saved:,.2f}\")\n",
    "    print(f\"  Net benefit: ${net_benefit:,.2f}\")\n",
    "    print(f\"  ROI: {(net_benefit / campaign_cost) * 100:.1f}%\" if campaign_cost > 0 else \"  ROI: N/A\")\n",
    "    \n",
    "    # CLV-based resource allocation\n",
    "    y_true_clv = predictions_df['Actual_CLV']\n",
    "    y_pred_clv = predictions_df['Predicted_CLV']\n",
    "    \n",
    "    # Segment customers by predicted CLV\n",
    "    clv_quartiles = pd.qcut(y_pred_clv, 4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    # Calculate misallocation costs\n",
    "    service_costs = {'Low': 10, 'Medium': 25, 'High': 50, 'Very High': 100}  # Service cost per customer\n",
    "    \n",
    "    total_allocation_cost = 0\n",
    "    optimal_allocation_cost = 0\n",
    "    \n",
    "    for quartile in ['Low', 'Medium', 'High', 'Very High']:\n",
    "        predicted_in_quartile = (clv_quartiles == quartile).sum()\n",
    "        actual_quartile = pd.qcut(y_true_clv, 4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "        actual_in_quartile = (actual_quartile == quartile).sum()\n",
    "        \n",
    "        total_allocation_cost += predicted_in_quartile * service_costs[quartile]\n",
    "        optimal_allocation_cost += actual_in_quartile * service_costs[quartile]\n",
    "    \n",
    "    allocation_efficiency = (optimal_allocation_cost / total_allocation_cost) * 100 if total_allocation_cost > 0 else 100\n",
    "    \n",
    "    print(f\"\\nüíé CLV-BASED RESOURCE ALLOCATION:\")\n",
    "    print(f\"  Current allocation cost: ${total_allocation_cost:,.2f}\")\n",
    "    print(f\"  Optimal allocation cost: ${optimal_allocation_cost:,.2f}\")\n",
    "    print(f\"  Allocation efficiency: {allocation_efficiency:.1f}%\")\n",
    "    print(f\"  Potential savings: ${total_allocation_cost - optimal_allocation_cost:,.2f}\")\n",
    "    \n",
    "    # Overall business impact summary\n",
    "    total_customers = len(predictions_df)\n",
    "    total_revenue = y_true_clv.sum()\n",
    "    \n",
    "    print(f\"\\nüìä OVERALL BUSINESS IMPACT:\")\n",
    "    print(f\"  Total customers analyzed: {total_customers:,}\")\n",
    "    print(f\"  Total customer lifetime value: ${total_revenue:,.2f}\")\n",
    "    print(f\"  Churn prevention net benefit: ${net_benefit:,.2f}\")\n",
    "    print(f\"  Resource allocation savings: ${total_allocation_cost - optimal_allocation_cost:,.2f}\")\n",
    "    print(f\"  Combined annual impact: ${net_benefit + (total_allocation_cost - optimal_allocation_cost):,.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'churn_prevention': {\n",
    "            'high_risk_customers': high_risk_customers,\n",
    "            'campaign_cost': campaign_cost,\n",
    "            'revenue_saved': revenue_saved,\n",
    "            'net_benefit': net_benefit,\n",
    "            'roi': (net_benefit / campaign_cost) * 100 if campaign_cost > 0 else 0\n",
    "        },\n",
    "        'resource_allocation': {\n",
    "            'current_cost': total_allocation_cost,\n",
    "            'optimal_cost': optimal_allocation_cost,\n",
    "            'efficiency': allocation_efficiency,\n",
    "            'savings': total_allocation_cost - optimal_allocation_cost\n",
    "        },\n",
    "        'total_impact': net_benefit + (total_allocation_cost - optimal_allocation_cost)\n",
    "    }\n",
    "\n",
    "# Assess business impact\n",
    "business_impact = assess_business_impact(predictions_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize business impact\n",
    "def visualize_business_impact(business_impact, predictions_df):\n",
    "    \"\"\"Visualize business impact assessment\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=['Churn Prevention ROI', 'Resource Allocation Efficiency', \n",
    "                       'CLV Distribution by Risk Level', 'Cost-Benefit Analysis']\n",
    "    )\n",
    "    \n",
    "    # Churn Prevention ROI\n",
    "    churn_data = business_impact['churn_prevention']\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=['Campaign Cost', 'Revenue Saved', 'Net Benefit'],\n",
    "            y=[churn_data['campaign_cost'], churn_data['revenue_saved'], churn_data['net_benefit']],\n",
    "            name='Churn Prevention'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Resource Allocation Efficiency\n",
    "    allocation_data = business_impact['resource_allocation']\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=['Current Cost', 'Optimal Cost'],\n",
    "            y=[allocation_data['current_cost'], allocation_data['optimal_cost']],\n",
    "            name='Allocation Cost'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # CLV Distribution by Risk Level\n",
    "    y_proba_churn = predictions_df['Churn_Probability']\n",
    "    y_pred_clv = predictions_df['Predicted_CLV']\n",
    "    \n",
    "    high_risk = y_pred_clv[y_proba_churn >= 0.7]\n",
    "    low_risk = y_pred_clv[y_proba_churn < 0.3]\n",
    "    medium_risk = y_pred_clv[(y_proba_churn >= 0.3) & (y_proba_churn < 0.7)]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Box(y=high_risk, name='High Risk', boxpoints='outliers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Box(y=medium_risk, name='Medium Risk', boxpoints='outliers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Box(y=low_risk, name='Low Risk', boxpoints='outliers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Cost-Benefit Summary\n",
    "    total_benefits = churn_data['revenue_saved'] + allocation_data['savings']\n",
    "    total_costs = churn_data['campaign_cost']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=['Total Benefits', 'Total Costs', 'Net Impact'],\n",
    "            y=[total_benefits, total_costs, business_impact['total_impact']],\n",
    "            name='Overall Impact'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"Business Impact Analysis Dashboard\")\n",
    "    fig.show()\n",
    "\n",
    "# Visualize business impact\n",
    "visualize_business_impact(business_impact, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Deployment Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment recommendations and monitoring plan\n",
    "def generate_deployment_recommendations(churn_metrics, clv_metrics, stability_results, business_impact):\n",
    "    \"\"\"Generate comprehensive deployment recommendations\"\"\"\n",
    "    \n",
    "    print(\"üöÄ DEPLOYMENT RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Model readiness assessment\n",
    "    churn_ready = (\n",
    "        churn_metrics['auc'] >= 0.75 and \n",
    "        churn_metrics['precision'] >= 0.6 and \n",
    "        stability_results['stability']['churn_stable']\n",
    "    )\n",
    "    \n",
    "    clv_ready = (\n",
    "        clv_metrics['r2'] >= 0.6 and \n",
    "        clv_metrics['mape'] <= 30 and \n",
    "        stability_results['stability']['clv_stable']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìã MODEL READINESS ASSESSMENT:\")\n",
    "    print(f\"  Churn Model: {'‚úÖ Ready for Production' if churn_ready else '‚ö†Ô∏è Needs Improvement'}\")\n",
    "    print(f\"  CLV Model: {'‚úÖ Ready for Production' if clv_ready else '‚ö†Ô∏è Needs Improvement'}\")\n",
    "    \n",
    "    # Deployment strategy\n",
    "    print(f\"\\nüéØ RECOMMENDED DEPLOYMENT STRATEGY:\")\n",
    "    \n",
    "    if churn_ready and clv_ready:\n",
    "        print(\"  Strategy: Full Production Deployment\")\n",
    "        print(\"  Timeline: 2-4 weeks\")\n",
    "        print(\"  Risk Level: Low\")\n",
    "        deployment_approach = \"full_deployment\"\n",
    "    elif churn_ready or clv_ready:\n",
    "        print(\"  Strategy: Phased Deployment (Ready models first)\")\n",
    "        print(\"  Timeline: 4-6 weeks\")\n",
    "        print(\"  Risk Level: Medium\")\n",
    "        deployment_approach = \"phased_deployment\"\n",
    "    else:\n",
    "        print(\"  Strategy: Pilot Testing Required\")\n",
    "        print(\"  Timeline: 6-12 weeks\")\n",
    "        print(\"  Risk Level: High\")\n",
    "        deployment_approach = \"pilot_testing\"\n",
    "    \n",
    "    # Performance thresholds for monitoring\n",
    "    print(f\"\\nüìä PERFORMANCE MONITORING THRESHOLDS:\")\n",
    "    print(f\"  Churn Model:\")\n",
    "    print(f\"    - AUC Score: Monitor if < {churn_metrics['auc'] * 0.95:.3f}\")\n",
    "    print(f\"    - Precision: Monitor if < {churn_metrics['precision'] * 0.95:.3f}\")\n",
    "    print(f\"    - Prediction Volume: Monitor if churn rate changes by >20%\")\n",
    "    \n",
    "    print(f\"  CLV Model:\")\n",
    "    print(f\"    - R¬≤ Score: Monitor if < {clv_metrics['r2'] * 0.95:.3f}\")\n",
    "    print(f\"    - MAPE: Monitor if > {clv_metrics['mape'] * 1.1:.1f}%\")\n",
    "    print(f\"    - Bias: Monitor if mean error > ${abs(clv_metrics['errors'].mean()) * 2:.2f}\")\n",
    "    \n",
    "    # Business value validation\n",
    "    print(f\"\\nüíº BUSINESS VALUE VALIDATION:\")\n",
    "    roi = business_impact['churn_prevention']['roi']\n",
    "    total_impact = business_impact['total_impact']\n",
    "    \n",
    "    if roi > 200 and total_impact > 10000:\n",
    "        print(f\"  ‚úÖ High Business Value: ROI = {roi:.1f}%, Total Impact = ${total_impact:,.2f}\")\n",
    "        print(f\"  Recommendation: Prioritize deployment and scale quickly\")\n",
    "    elif roi > 100 or total_impact > 5000:\n",
    "        print(f\"  ‚ö° Medium Business Value: ROI = {roi:.1f}%, Total Impact = ${total_impact:,.2f}\")\n",
    "        print(f\"  Recommendation: Deploy with careful monitoring\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Limited Business Value: ROI = {roi:.1f}%, Total Impact = ${total_impact:,.2f}\")\n",
    "        print(f\"  Recommendation: Re-evaluate model parameters or business assumptions\")\n",
    "    \n",
    "    # Infrastructure recommendations\n",
    "    print(f\"\\nüèóÔ∏è INFRASTRUCTURE RECOMMENDATIONS:\")\n",
    "    print(f\"  - Real-time scoring: Required for churn prevention\")\n",
    "    print(f\"  - Batch processing: Suitable for CLV updates (monthly)\")\n",
    "    print(f\"  - Model storage: Version control and rollback capability\")\n",
    "    print(f\"  - Monitoring: Automated alerts for performance degradation\")\n",
    "    print(f\"  - A/B testing: Compare model performance against baseline\")\n",
    "    \n",
    "    # Risk mitigation\n",
    "    print(f\"\\n‚ö†Ô∏è RISK MITIGATION STRATEGIES:\")\n",
    "    print(f\"  - Gradual rollout: Start with 10% of customers, increase weekly\")\n",
    "    print(f\"  - Champion/Challenger: Keep current methods as backup\")\n",
    "    print(f\"  - Human oversight: Review high-impact predictions manually\")\n",
    "    print(f\"  - Regular retraining: Monthly model updates with new data\")\n",
    "    print(f\"  - Performance tracking: Daily dashboard monitoring\")\n",
    "    \n",
    "    # Success metrics\n",
    "    print(f\"\\nüéØ SUCCESS METRICS TO TRACK:\")\n",
    "    print(f\"  Business Metrics:\")\n",
    "    print(f\"    - Customer retention rate improvement\")\n",
    "    print(f\"    - Revenue per customer increase\")\n",
    "    print(f\"    - Marketing campaign efficiency\")\n",
    "    print(f\"    - Customer satisfaction scores\")\n",
    "    \n",
    "    print(f\"  Technical Metrics:\")\n",
    "    print(f\"    - Model prediction accuracy\")\n",
    "    print(f\"    - Prediction latency (< 100ms target)\")\n",
    "    print(f\"    - System uptime (> 99.9% target)\")\n",
    "    print(f\"    - Data quality scores\")\n",
    "    \n",
    "    return {\n",
    "        'churn_ready': churn_ready,\n",
    "        'clv_ready': clv_ready,\n",
    "        'deployment_approach': deployment_approach,\n",
    "        'business_value': 'high' if roi > 200 and total_impact > 10000 else 'medium' if roi > 100 or total_impact > 5000 else 'limited'\n",
    "    }\n",
    "\n",
    "# Generate deployment recommendations\n",
    "deployment_rec = generate_deployment_recommendations(churn_metrics, clv_metrics, stability_results, business_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation summary\n",
    "def create_evaluation_summary(churn_metrics, clv_metrics, stability_results, business_impact, deployment_rec):\n",
    "    \"\"\"Create a comprehensive evaluation summary\"\"\"\n",
    "    \n",
    "    summary_data = {\n",
    "        'Model': ['Churn Prediction', 'CLV Prediction'],\n",
    "        'Primary_Metric': [f\"{churn_metrics['auc']:.3f} (AUC)\", f\"{clv_metrics['r2']:.3f} (R¬≤)\"],\n",
    "        'Secondary_Metric': [f\"{churn_metrics['precision']:.3f} (Precision)\", f\"{clv_metrics['rmse']:.2f} (RMSE)\"],\n",
    "        'Stability': ['Stable' if stability_results['stability']['churn_stable'] else 'Unstable',\n",
    "                     'Stable' if stability_results['stability']['clv_stable'] else 'Unstable'],\n",
    "        'Business_Impact': [f\"${business_impact['churn_prevention']['net_benefit']:,.0f}\",\n",
    "                          f\"${business_impact['resource_allocation']['savings']:,.0f}\"],\n",
    "        'Deployment_Ready': ['‚úÖ' if deployment_rec['churn_ready'] else '‚ö†Ô∏è',\n",
    "                            '‚úÖ' if deployment_rec['clv_ready'] else '‚ö†Ô∏è']\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nüìã COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Overall recommendation\n",
    "    print(f\"\\nüéØ OVERALL RECOMMENDATION:\")\n",
    "    if deployment_rec['churn_ready'] and deployment_rec['clv_ready']:\n",
    "        print(f\"  ‚úÖ PROCEED WITH FULL DEPLOYMENT\")\n",
    "        print(f\"  Both models meet production standards and show strong business impact.\")\n",
    "    elif deployment_rec['churn_ready'] or deployment_rec['clv_ready']:\n",
    "        print(f\"  ‚ö° PROCEED WITH PHASED DEPLOYMENT\")\n",
    "        ready_model = \"Churn\" if deployment_rec['churn_ready'] else \"CLV\"\n",
    "        print(f\"  Deploy {ready_model} model first, improve the other before deployment.\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è ADDITIONAL DEVELOPMENT REQUIRED\")\n",
    "        print(f\"  Models need improvement before production deployment.\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Create evaluation summary\n",
    "evaluation_summary = create_evaluation_summary(churn_metrics, clv_metrics, stability_results, business_impact, deployment_rec)\n",
    "\n",
    "# Save evaluation results\n",
    "try:\n",
    "    # Save comprehensive evaluation summary\n",
    "    evaluation_summary.to_csv('../reports/analysis/05_model_evaluation_summary.csv', index=False)\n",
    "    print(\"üíæ Evaluation summary saved to ../reports/analysis/05_model_evaluation_summary.csv\")\n",
    "    \n",
    "    # Save business impact analysis\n",
    "    business_impact_df = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Churn Prevention - High Risk Customers',\n",
    "            'Churn Prevention - Campaign Cost',\n",
    "            'Churn Prevention - Revenue Saved',\n",
    "            'Churn Prevention - Net Benefit',\n",
    "            'Churn Prevention - ROI (%)',\n",
    "            'Resource Allocation - Current Cost',\n",
    "            'Resource Allocation - Optimal Cost',\n",
    "            'Resource Allocation - Efficiency (%)',\n",
    "            'Resource Allocation - Savings',\n",
    "            'Total Annual Impact'\n",
    "        ],\n",
    "        'Value': [\n",
    "            business_impact['churn_prevention']['high_risk_customers'],\n",
    "            business_impact['churn_prevention']['campaign_cost'],\n",
    "            business_impact['churn_prevention']['revenue_saved'],\n",
    "            business_impact['churn_prevention']['net_benefit'],\n",
    "            business_impact['churn_prevention']['roi'],\n",
    "            business_impact['resource_allocation']['current_cost'],\n",
    "            business_impact['resource_allocation']['optimal_cost'],\n",
    "            business_impact['resource_allocation']['efficiency'],\n",
    "            business_impact['resource_allocation']['savings'],\n",
    "            business_impact['total_impact']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    business_impact_df.to_csv('../reports/analysis/05_business_impact_assessment.csv', index=False)\n",
    "    print(\"üíæ Business impact assessment saved to ../reports/analysis/05_business_impact_assessment.csv\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model evaluation completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save evaluation results: {e}\")\n",
    "    print(\"üìä Evaluation completed - results available in notebook\")\n",
    "\n",
    "# Final comprehensive summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéä MODEL EVALUATION COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ Churn Model Performance: AUC = {churn_metrics['auc']:.3f}, Precision = {churn_metrics['precision']:.3f}\")\n",
    "print(f\"‚úÖ CLV Model Performance: R¬≤ = {clv_metrics['r2']:.3f}, RMSE = ${clv_metrics['rmse']:.2f}\")\n",
    "print(f\"‚úÖ Model Stability: Both models {'passed' if stability_results['stability']['churn_stable'] and stability_results['stability']['clv_stable'] else 'need attention'}\")\n",
    "print(f\"‚úÖ Business Impact: ${business_impact['total_impact']:,.2f} annual value\")\n",
    "print(f\"‚úÖ Deployment Status: {deployment_rec['deployment_approach'].replace('_', ' ').title()}\")\n",
    "print(f\"\\nüöÄ Next Steps: Implement deployment plan and monitoring framework\")\n",
    "print(f\"üìà Expected ROI: {business_impact['churn_prevention']['roi']:.1f}% within first year\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}